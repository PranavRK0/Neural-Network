This repository contains a complete implementation of a simple neural network built from the ground up using NumPy. The project focuses on understanding the foundational principles of neural networks, including forward propagation, backpropagation, and gradient descent, without relying on external deep learning libraries.

Neural Network Architecture:
Inputs: 2 features per data point.
Hidden Layer: 1 layer with 2 neurons.
Output Layer: 1 neuron for binary classification.

Key Features:
Activation Function: Implements sigmoid and its derivative for activations.
Loss Function: Uses Mean Squared Error (MSE) for evaluating predictions.
Training: Gradient-based weight and bias updates via backpropagation. Supports configurable learning rate and number of epochs.
Predictions: Demonstrates the ability to predict outputs for unseen data.
